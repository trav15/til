# S3

- [General info](#general)
- [Storage Classes](#s3-storage-classes)
    - [Glacier specifics](#glacier-specifics)
- [Object Lock](#object-lock)
- [Signed URLS](#signed-urls)
- [Encryption](#encryption)
- [Static Website Hosting](#static-website-hosting)
- [Access Points](#access-points)
- [S3 Notifications](#s3-notifications)
- [File Gateway](#file-gateway)
- [Requester Pays](#requester-pays)
- [Amazon Athena](#amazon-athena-aa)

## General
- Hot storage refers to the storage that keeps frequently accessed data (hot data). 
- Warm storage refers to the storage that keeps less frequently accessed data (warm data).
- Cold storage refers to the storage that keeps rarely accessed data (cold data). 

In terms of pricing, the colder the data, the cheaper it is to store, and the costlier it is to access when needed.

## S3 Storage Classes

- 99.999999999% (11 9's) durability
- 99.99% availability: S3 Standard, Glacier Flexible Retrieval, and Glacier Deep Archive
    - 99.9% 
        - Intelligent-Tiering
        - Standard IA
        - Glacier Instant Retrieval
    - 99.5%
        - One Zone-IA 

|Class | Use |
| ---- | --- |
| S3 Standard | General-purpose storage of frequently accessed data. |
| Standard-IA | For long-lived, but less frequently accessed data. It stores the object data redundantly across multiple geographically separated AZs. |
| One Zone-IA | Only one AZ. Less expensive than STANDARD-IA, but data is not resilient to the physical loss of the AZ |
| Intelligent Tiering | Delivers automatic cost savings by moving data between two access tiers — Frequent Access and Infrequent Access |
| Glacier Instant Retrieval | For long-lived data that are rarely accessed and must be retrieved in milliseconds |
| Glacier Flexible Retrieval | For archive data that is accessed once or twice per year; retrieval in minutes to hours. Formerly **S3 Glacier**. See more [Glacier specifics](#glacier-specifics).|
| Glacier Deep Archive | For data that is accessed rarely in a year; retrieval in 12-48 hours. Lowest cost storage.|
| S3 on Outposts | Brings S3 APIs and features to your own **on-premises AWS Outposts environment**. Designed to durably and redundantly store data across multiple devices and servers on your Outposts |

- For S3 Standard, S3 Standard-IA, and Glacier storage classes, your objects are automatically stored across multiple devices spanning a minimum of three Availability Zones.
- S3 Intelligent-Tiering monitors access patterns and moves objects that have not been accessed for 30 consecutive days to the infrequent access tier. 
    - If an object in the infrequent access tier is accessed later, it is automatically moved back to the frequent access tier.
    - S3 Intelligent-Tiering supports the archive access tier.  If the objects haven’t been accessed for 90 consecutive days, it will be moved to the archive access tier. After 180 consecutive days of no access, it is automatically moved to the deep archive access tier.
    - There are no retrieval fees in S3 Intelligent-Tiering.

### Glacier specifics

- Archived objects are not available for real-time access. You must first restore the objects before you can access them.
- You cannot specify GLACIER as the storage class at the time that you create an object.
- Glacier objects are visible through S3 only.
- Retrieval Options
    - **Expedited** – allows you to quickly access your data when occasional urgent requests for a subset of archives are required. For all but the largest archived objects, data accessed are typically *made available within 1–5 minutes*. There are two types of Expedited retrievals: **On-Demand requests** are similar to EC2 On-Demand instances and are available most of the time. **Provisioned requests** are guaranteed to be available when you need them.
    - **Standard** – allows you to access any of your archived objects *within several hours*. Standard retrievals *typically complete within 3–5 hours*. This is the *default option* for retrieval requests that do not specify the retrieval option.
    - **Bulk** – Glacier’s *lowest-cost retrieval option*, enabling you to retrieve large amounts, even petabytes, of data inexpensively in a day. Bulk retrievals typically complete *within 5–12 hours*.

## Lifecycle configuration

[Lifecycle configuration](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html) enables you to specify the lifecycle management of objects in a bucket. The configuration is a set of one or more rules, where each rule defines an action for Amazon S3 to apply to a group of objects. These actions can be classified as follows:
- **Transition actions** – In which you define when objects transition to another storage class. For example, you may choose to transition objects to the STANDARD_IA (IA, for infrequent access) storage class 30 days after creation or archive objects to the GLACIER storage class one year after creation.
- **Expiration actions**** – In which you specify when the objects expire. Then Amazon S3 deletes the expired objects on your behalf.

## Object Lock

With S3 Object Lock, you can store objects using a write-once-read-many (WORM) model. ***Object Lock can help prevent objects from being deleted or overwritten for a fixed amount of time or indefinitely***. You can use Object Lock to help meet regulatory requirements that require WORM storage or to simply add another layer of protection against object changes and deletion.
- Before you lock any objects, ***you have to enable a bucket to use S3 Object Lock when you create a bucket***. After you enable Object Lock on a bucket, you can lock objects in that bucket. When you create a bucket with Object Lock enabled, you can’t disable Object Lock or suspend versioning for that bucket.

S3 Object Lock provides two retention modes: Governance mode and Compliance mode
- In **governance mode**, users can’t overwrite or delete an object version or alter its lock settings *unless they have special permissions*. With governance mode, you protect objects against being deleted by most users, but you can still grant some users permission to alter the retention settings or delete the object if necessary. You can also use governance mode to test retention-period settings before creating a compliance-mode retention period.
- In **compliance mode**, a protected object version *can’t be overwritten or deleted by ANY USER*, including the root user in your AWS account. When an object is locked in compliance mode, its *retention mode can’t be changed, and its retention period can’t be shortened*. Compliance mode helps ensure that an object version can’t be overwritten or deleted for the duration of the retention period.

With Object Lock, you can also place a **legal hold** on an object version. Like a retention period, a legal hold prevents an object version from being overwritten or deleted. However, a legal hold doesn’t have an associated retention period and remains in effect until removed. Legal holds can be freely placed and removed by any user who has the `s3:PutObjectLegalHold` permission.
- *​Legal holds are independent from retention periods*. As long as the bucket that contains the object has Object Lock enabled, you can place and remove legal holds regardless of whether the specified object version has a retention period set. ***Placing a legal hold on an object version doesn’t affect the retention mode or retention period for that object version***. You cannot set a time period for a legal hold. You can only do this using the “retention period” option.

## Signed URLs

In Amazon S3, *all objects are private by default*. Only the object owner has permission to access these objects. However, the object owner can optionally share objects with others by creating a **pre-signed URL**, using their own security credentials, *to grant time-limited permission to download the objects*.
- When you create a pre-signed URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object) and expiration date and time. The pre-signed URLs are valid only for the specified duration. Anyone who receives the pre-signed URL can then access the object.

## Encryption

***Server-side encryption protects data at rest***. If you use Server-Side Encryption with Amazon S3-Managed Encryption Keys (SSE-S3), Amazon S3 will encrypt each object with a unique key and as an additional safeguard, it encrypts the key itself with a master key that it rotates regularly. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data.
- ***If you need server-side encryption for all of the objects that are stored in a bucket, use a bucket policy***. For example, the following bucket policy denies permissions to upload an object unless the request includes the `x-amz-server-side-encryption` header to request server-side encryption:
- However, if you choose to use server-side encryption with customer-provided encryption keys (SSE-C), you must provide encryption key information using the following request headers:
    - `x-amz-server-side-encryption-customer-algorithm`
    - `x-amz-server-side-encryption-customer-key`
    - `x-amz-server-side-encryption-customer-key-MD5`

## Static website hosting

Below are the prerequisites for routing traffic to a website that is hosted in an Amazon S3 Bucket:
- An S3 bucket that is configured to host a static website. The ***bucket must have the same name as your domain or subdomain***. For example, if you want to use the subdomain portal.thisexample.com, the name of the bucket must be portal.thisexample.com.
- A registered domain name. You can use Route 53 as your domain registrar, or you can use a different registrar.
- *Route 53 as the DNS service for the domain*. If you register your domain name by using Route 53, we automatically configure Route 53 as the DNS service for the domain.

## Cross-Region Replication [^crr]

When you upload your data in S3, your objects are redundantly stored on multiple devices across multiple facilities *within the region only*, where you created the bucket. Thus, if there is an outage on the entire region, your S3 bucket will be unavailable if you do not enable Cross-Region Replication. 
- Note that an Availability Zone (AZ) is more related with Amazon EC2 instances rather than Amazon S3 so if there is any outage in the AZ, the S3 bucket is usually not affected but only the EC2 instances deployed on that zone.

To enable the cross-region replication feature in S3, the following items should be met:
- The source and destination buckets *must have versioning enabled*.
- The source and destination buckets *must be in different AWS Regions*.
- Amazon S3 must have permission to replicate objects from that source bucket to the destination bucket on your behalf.

[^crr]: [AWS docs S3 Cross-Region Replication](https://aws.amazon.com/blogs/aws/new-cross-region-replication-for-amazon-s3/)

## Access Points

**Amazon S3 access points** simplify data access for any AWS service or customer application that stores data in S3. Access points are named network endpoints that are attached to buckets that you can use to perform S3 object operations, such as GetObject and PutObject.

*Each access point has distinct permissions and network controls* that S3 applies for any request that is made through that access point. *Each access point enforces a customized access point policy* that works in conjunction with the bucket policy that is attached to the underlying bucket. You can configure any access point to accept requests only from a virtual private cloud (VPC) to restrict Amazon S3 data access to a private network. You can also configure custom block public access settings for each access point.

You can also use **Amazon S3 Multi-Region Access Points** to provide a *global endpoint* that applications can use to fulfill requests from S3 buckets located in multiple AWS Regions. *You can use Multi-Region Access Points to build multi-Region applications with the same simple architecture used in a single Region, and then run those applications anywhere in the world*. Instead of sending requests over the congested public internet, Multi-Region Access Points provide built-in network resilience with acceleration of internet-based requests to Amazon S3. Application requests made to a Multi-Region Access Point global endpoint use AWS Global Accelerator to automatically route over the AWS global network to the S3 bucket with the lowest network latency.

## S3 Notifications

**S3 event notifications** feature enables you to receive notifications when certain events happen in your bucket. *To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications*. You store this configuration in the notification subresource that is associated with a bucket.

Amazon S3 supports the following destinations where it can publish events:
- Amazon Simple Notification Service (Amazon SNS) topic
- Amazon Simple Queue Service (Amazon SQS) queue
- AWS Lambda

*Take note that Amazon S3 event notifications are designed to be delivered at least once and to __one destination only__. You cannot attach two or more SNS topics or SQS queues for S3 event notification*. Therefore, you must send the event notification to Amazon SNS. In Amazon SNS, the **fanout scenario** is when a message published to an SNS topic is replicated and pushed to multiple endpoints, such as Amazon SQS queues, HTTP(S) endpoints, and Lambda functions. This allows for parallel asynchronous processing.

## File Gateway

A file gateway supports a file interface into Amazon Simple Storage Service (Amazon S3) and combines a service and a virtual software appliance. By using this combination, you can store and retrieve objects in Amazon S3 using industry-standard file protocols such as Network File System (NFS) and Server Message Block (SMB). The software appliance, or gateway, is deployed into your on-premises environment as a virtual machine (VM) running on VMware ESXi, Microsoft Hyper-V, or Linux Kernel-based Virtual Machine (KVM) hypervisor.

The gateway provides access to objects in S3 as files or file share mount points. With a file gateway, you can do the following:
- You can store and retrieve files directly using the NFS version 3 or 4.1 protocol.
- You can store and retrieve files directly using the SMB file system version, 2 and 3 protocol.
- You can access your data directly in Amazon S3 from any AWS Cloud application or service.
- You can manage your Amazon S3 data using lifecycle policies, cross-region replication, and versioning. You can think of a file gateway as a file system mount on S3.

## Requester Pays

With **Requester Pays** buckets, the requester instead of the bucket owner pays the cost of the request and the data download from the bucket. *The bucket owner always pays the cost of storing data*.

Typically, you configure buckets to be Requester Pays buckets when you want to share data but not incur charges associated with others accessing the data. Take note that if you enable Requester Pays on a bucket, anonymous access to that bucket will not be allowed anymore.
- When the requester assumes an AWS Identity and Access Management (IAM) role before making their request, the account to which the role belongs is charged for the request.
- After you configure a bucket to be a Requester Pays bucket, requesters must include `x-amz-request-payer` in their API request header, for DELETE, GET, HEAD, POST, and PUT requests, or as a parameter in a REST request to show that they understand that they will be charged for the request and the data download.

## Amazon Athena [^aa]

Amazon Athena is an interactive query service that makes it easy to _analyze data directly in Amazon S3 using standard SQL_. With a few actions in the AWS Management Console, you can point Athena at your data stored in Amazon S3 and begin using standard SQL to run ad-hoc queries and get results in seconds.

Athena is serverless, so there is no infrastructure to set up or manage, and you pay only for the queries you run. Athena scales automatically—executing queries in parallel—so results are fast, even with large datasets and complex queries.

Athena helps you analyze unstructured, semi-structured, and structured data stored in Amazon S3. Examples include CSV, JSON, or columnar data formats such as Apache Parquet and Apache ORC. You can use Athena to run ad-hoc queries using ANSI SQL without the need to aggregate or load the data into Athena.

Athena integrates with the **AWS Glue Data Catalog**, which offers a persistent metadata store for your data in Amazon S3. This allows you to create tables and query data in Athena based on a central metadata store available throughout your Amazon Web Services account and integrated with the *ETL and data discovery features of AWS Glue*.
- [TD Athena Cheat Sheet](https://tutorialsdojo.com/amazon-athena/)

[^aa]: [Amazon Athena docs](https://docs.aws.amazon.com/athena/latest/ug/when-should-i-use-ate.html)

## *Resources*

- [Tutorials Dojo S3 cheat sheet](https://tutorialsdojo.com/amazon-s3/)
- [AWS S3 Storage Classes docs](https://aws.amazon.com/s3/storage-classes/)